# jobs

spawn container(s) and complete once the task is done, no restart like a pod.

A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. Suspending a Job will delete its active Pods until the Job is resumed again.


## single pod

```yaml
apiVersion: batch/v1
kind: Job
metadata:
    name: math-add-job
spec:
    template: 
        <pod-definition-spec>
        restartPolicy: Never
```

create job with the above manifest

```
    kubectl create -f math-add-job.yaml    
```

or 

```
    kubectl create job math-add-job --image=image-name --dry-run=client -o yaml > math-add-job.yaml
```

```
    kubectl get jobs
    kubectl get pods
    kubectl logs math-add-job-9jshd
    kubectl delete job math-add-job
```

once we delete the job, the pod that was created will also get deleted


## multi pod

the desired number of pods are defined under spec.completion.

### sequentially / non-parallel jobs

* normally, only one Pod is started, unless the Pod fails.
* the Job is complete as soon as its Pod terminates successfully.
* For a non-parallel Job, you can leave both .spec.completions and .spec.parallelism unset. When both are unset, both are defaulted to 1.

the pods are created one after another when the previous spawned pod goes to a completion.

if there is a failure in the pod, a new one is spun and taken to completion until we have the desired number of pods run sucessfully completion.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
    name: math-add-job
spec:
    completions: 3
    template: 
        <pod-definition-spec>
        restartPolicy: Never
```

### parallelism / parallel job with fixed completion count

* specify a non-zero positive value for .spec.completions.
* the Job represents the overall task, and is complete when there are .spec.completions successful Pods.
* when using .spec.completionMode="Indexed", each Pod gets a different index in the range 0 to .spec.completions-1.
* For a fixed completion count Job, you should set .spec.completions to the number of completions needed. You can set .spec.parallelism, or leave it unset and it will default to 1.
* For fixed completion count Jobs, the actual number of pods running in parallel will not exceed the number of remaining completions. Higher values of .spec.parallelism are effectively ignored.

instead of one after another, we can run them parallely

spec.parallelism 

number of pods specified in parallelism are spawned at once, if any of those fail, the pods are then created one by one until we have the desired number of completions met.

```yaml
apiVersion: batch/v1
kind: Job
metadata:
    name: math-add-job
spec:
    parallelism: 3
    completions: 3
    template: 
        <pod-definition-spec>
        restartPolicy: Never
```

### work queue job / no completions, just parallelism

* do not specify .spec.completions, default to .spec.parallelism.
* the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.
* each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.
* when any Pod from the Job terminates with success, no new Pods are created.
* once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.
* once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.
* For work queue Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.

For a work queue Job, you must leave .spec.completions unset, and set .spec.parallelism to a non-negative integer.


## restartPolicy

Only a RestartPolicy equal to Never or OnFailure is allowed.

OnFailure - restarts the container in the pod when the container fails.
Never - does not restart the container when it fails, it RESTARTS THE POD INSTEAD.

### onFailure

### Never

## backoffLimit 

There are situations where you want to fail a Job after some amount of retries due to a logical error in configuration etc. To do so, set .spec.backoffLimit to specify the number of retries before considering a Job as failed. The back-off limit is set by default to 6. Failed Pods associated with the Job are recreated by the Job controller with an exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.

The number of retries is calculated in two ways:

* The number of Pods with .status.phase = "Failed".
* When using restartPolicy = "OnFailure", the number of retries in all the containers of Pods with .status.phase equal to Pending or Running.

If either of the calculations reaches the .spec.backoffLimit, the Job is considered failed.

## ttlSecondsAfterFinished

after the ttl is expired, the job object is deleted, and of course so is the pod along with it

## pod selector

the .spec.selector field is optional. In almost all cases you should not specify it. 

## suspending job

When a Job is created, the Job controller will immediately begin creating Pods to satisfy the Job's requirements and will continue to do so until the Job is complete. However, you may want to temporarily suspend a Job's execution and resume it later, or start Jobs in suspended state and have a custom controller decide later when to start them.

To suspend a Job, you can update the .spec.suspend field of the Job to true; later, when you want to resume it again, update it to false. Creating a Job with .spec.suspend set to true will create it in the suspended state.

An example Job definition in the suspended state can be like so:

    kubectl get job myjob -o yaml
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  suspend: true
  parallelism: 1
  completions: 5
  template:
    spec:
      ...
```


You can also toggle Job suspension by patching the Job using the command line.

Suspend an active Job:

    kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":true}}'

Resume a suspended Job:

    kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":false}}'

The Job's status can be used to determine if a Job is suspended or has been suspended in the past:

    kubectl get jobs/myjob -o yaml
```yaml
apiVersion: batch/v1
kind: Job
# .metadata and .spec omitted
status:
  conditions:
  - lastProbeTime: "2021-02-05T13:14:33Z"
    lastTransitionTime: "2021-02-05T13:14:33Z"
    status: "True"
    type: Suspended
  startTime: "2021-02-05T13:13:48Z"
```

The Job condition of type "Suspended" with status "True" means the Job is suspended; the lastTransitionTime field can be used to determine how long the Job has been suspended for. If the status of that condition is "False", then the Job was previously suspended and is now running. If such a condition does not exist in the Job's status, the Job has never been stopped.

Events are also created when the Job is suspended and resumed:

    kubectl describe jobs/myjob

    Name:           myjob
    ...
    Events:
    Type    Reason            Age   From            Message
    ----    ------            ----  ----            -------
    Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
    Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
    Normal  Suspended         11m   job-controller  Job suspended
    Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
    Normal  Resumed           3s    job-controller  Job resumed

The last four events, particularly the "Suspended" and "Resumed" events, are directly a result of toggling the .spec.suspend field. In the time between these two events, we see that no Pods were created, but Pod creation restarted as soon as the Job was resumed