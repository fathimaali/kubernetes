# replicaset 

pods are encapsulated in a replicaset

* ensure high availability

    replication controller spawns up pods to match the desired number of pods in the node

    even if its just a single-pod node, the replication controller, spawns up a new pod when the existing one fails. 

* load balancing & scaling 

    scaling : horizontally scale up or down based on demand
    either up or down number of pods in a node
    or spin up or down nodes based on demand

    load balancing : replication controller also helps load balance the app among the several pods

**short for replicaset while using imperatively is rs**

### Create

yaml: 

```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata: 
        name: my-app-rs
        labels: 
            app: myapp
            type: front-end
    spec: 
        template: 
            metadata: 
                name: pod1
                labels: 
                    type: front-end
            spec: 
                containers:
                    - name: nginx
                      image: nginx
        replicas: 3
        selector: 
            matchLabels: 
                type: front-end
```

pod.yaml

```yaml
    metadata: 
        name: my-app-pod
        labels: 
            type: front-end
```

*selector* helps identify what pods are under this replicaset with labels
helps in managing pods that maybe had been created before the replicaset was created with those labels

### note: 

* if a pod exists with label XX, and a replicaset is created with matchLabels XX, then the replicaset auto manages this pod and makes this pod part of the replicaset
* if a pod is created with label XX, and a replicaset is already live with replicas=3, this pod will make total pods with labels XX = 4 and hence will get terminated by the replicaset
* if there are two replicasets rs1 and rs2, rs1 created before rs2, if a new pod is created with label XX and the above condition is matched, rs1 will terminate the pod and not rs2
* if rs1 (created before rs2) and rs2 replicasets exist, and a we delete rs1 and then create a new pod with label XX, now rs2 will terminate the new pod, since its already running the desired number of pods
### edit

in case we wanna edit the replicaset created, there are 2 ways
    - edit yaml and then > kubectl **replace** -f my-replicaset.yaml
    - > kubectl **scale** --replicas=6 -f my-replicaset.yaml 
    - > kubectl **scale** --replicas=6 replicaset my-replicaset 
        After you run the scale command, the number of replicas in your cluster will differ from that defined in your YAML's spec. replicas field. It's better practice to modify the YAML file instead, then re-apply it to your cluster.
    -> kubectl scale --replicas=6 **--current-replicas**=3 -f my-replicaset.yaml
        *current-replicas* is for security, to ensure that the current number is met, for preventing an accidental imperative scaling 

also when image of the container of the pod defined in a replicaset, if that's changed 
by either 

    -   > kubectl edit replicaset my-replica-set 
            this opens the yaml file as editor
    -   > kubectl delete rs my-replica-set
        > vi my-replica-set.yaml
            and edit the image
        > kubectl apply -f my-replica-set.yaml

    this would not delete the existing pods with the old image and recreate them
    this deletion needs to be done manually.
    
### delete

``` 
    > kubectl delete replicaset my-replicaset 
        this also deletes the underlying pods
```

# (replication controller) 

older tech than replicaset, but intended for the same purpose

yaml : 

```
    apiVersion: v1
    kind: ReplicationController
    metadata: 
        name: myapp-rc
        labels: 
            app: myapp
            type: front-end
    spec: 
        template: 
            metadata: 
                name: pod1
            spec: 
                containers:
                    - name: nginx
                      image: nginx
        replicas: 3
```

selector is also an attribute of replication controller, but if not mentioned, it will auto assume the labels from the pod's definition yaml
